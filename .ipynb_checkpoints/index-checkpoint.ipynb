{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bound-theme",
   "metadata": {},
   "source": [
    "# Database to Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-basics",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-experiment",
   "metadata": {},
   "source": [
    "In previous lessons, we moved through performing copying data from our OLTP tables to our OLAP tables.  But generally, we don't just use a different set of tables for OLAP, but a separate database (eg. Redshift) entirely.  In this lesson, we'll discuss how we'll migrate our data from one database to another.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-walnut",
   "metadata": {},
   "source": [
    "### Extracting to Another Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-decrease",
   "metadata": {},
   "source": [
    "Now remember that in production, our application will generally use an OLTP database to initially store our data.  In AWS, this is our RDS postgres instance. As users interact with our web application, data will be updated in RDS, our OLTP database.\n",
    "\n",
    "As an engineer, we'll then want to copy that data from our RDS database into a separate redshift living on a different location. \n",
    "\n",
    "How do we accomplish something like that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-prefix",
   "metadata": {},
   "source": [
    "> <img src=\"./sql-sql.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-burton",
   "metadata": {},
   "source": [
    "Well notice that there may not be the out of the box capability to move data directly from one database to the other.  So instead, we generally:\n",
    "\n",
    "1. Extract that data from our SQL database \n",
    "2. Copy the data into CSV files, stored on an ETL server, and\n",
    "3. Read the CSV files into our OLAP database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-burns",
   "metadata": {},
   "source": [
    "<img src=\"./sql_etl_server.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-consent",
   "metadata": {},
   "source": [
    "### AWS Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-royalty",
   "metadata": {},
   "source": [
    "Now in Amazon, we can take advantage of the fact that we can export data from our RDS instance directly into S3 buckets.  And once in those S3 buckets, our redshift server can read data from those buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-phoenix",
   "metadata": {},
   "source": [
    "> <img src=\"./rds_s3.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-cause",
   "metadata": {},
   "source": [
    "Now how do we issue these commands to copy data from RDS to S3, and from S3 over to redshift?  For that, we'll still have an ETL server which performs these SQL commands.  And if we're doing this all in AWS, that will be an EC2 machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-quick",
   "metadata": {},
   "source": [
    "> <img src=\"./rds_ec2_s3.jpg\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-convert",
   "metadata": {},
   "source": [
    "And finally, if we want to see the full picture, it looks like something below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-philippines",
   "metadata": {},
   "source": [
    "<img src=\"./full_data_flow.jpg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-indication",
   "metadata": {},
   "source": [
    "So this time, on our ETL server, we have Apache Airflow to regularly move data from the RDS instance, to S3 to Redshift.  And then once our data is in redshift, users can view that information either from a data dashboard, or it simply can be exported to a CSV file so that business analysts or data scientists can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-electronics",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-lyric",
   "metadata": {},
   "source": [
    "In this lesson, we saw an overview of the flow of our database from an OLTP database to an OLAP database.  As we saw, we'll use an ETL server to issue a command to select export data from our RDS instance and into CSV files that live on S3.  Then that same ETL server will take data from S3 and read it into the redshift OLAP database.  From there, this data can be read by users via CSV files or directly connected to a dashboard application like streamlit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
